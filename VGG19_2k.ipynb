{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43301f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "def convert_dcm_to_png_recursive(parent_dir, png_dir):\n",
    "    \"\"\"Recursively converts all DCM images within a directory and its subdirectories to PNG format.\"\"\"\n",
    "\n",
    "    for root, _, files in os.walk(parent_dir):  # Walk through all subdirectories\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".dcm\"):\n",
    "                dcm_path = os.path.join(root, filename)\n",
    "                png_path = os.path.join(png_dir, os.path.relpath(root, parent_dir),\n",
    "                                         os.path.splitext(filename)[0] + \".png\")\n",
    "\n",
    "                try:\n",
    "                    ds = pydicom.read_file(dcm_path)\n",
    "                    pixel_array = ds.pixel_array\n",
    "\n",
    "                    # Convert to grayscale if needed\n",
    "                    if len(pixel_array.shape) == 2:\n",
    "                        img = cv2.cvtColor(pixel_array, cv2.COLOR_GRAY2BGR)\n",
    "                    else:\n",
    "                        img = pixel_array\n",
    "\n",
    "                    os.makedirs(os.path.dirname(png_path), exist_ok=True)  # Create output directories if needed\n",
    "                    cv2.imwrite(png_path, img)  # Use cv2.imwrite for PNG as well\n",
    "                    print(f\"Converted {filename} to PNG format.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error converting {filename}: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "parent_dir = \"G:/ChineseCheck/Malignant\"  # Replace with the actual path\n",
    "png_dir = \"G:/ChineseCheck/MalignantP\"  # Replace with the desired output path\n",
    "convert_dcm_to_png_recursive(parent_dir, png_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "098f654f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images: 2000\n",
      "Shape of an Image: (224, 224, 3)\n",
      "Labels: [0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Image dimensions and batch size\n",
    "img_width, img_height = 224, 224\n",
    "batch_size = 8\n",
    "\n",
    "# ... (GPU availability and memory growth code from previous response)\n",
    "def label_images(directory, target_size=(img_width, img_height), max_images_per_class=1000):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_labels = {'Malignant': 0, 'Benign': 1}\n",
    "\n",
    "    for class_label, class_index in class_labels.items():\n",
    "        class_path = os.path.join(directory, class_label)\n",
    "        image_count = 0\n",
    "        for root, _, files in os.walk(class_path):\n",
    "            for filename in files:\n",
    "                if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                    if image_count < max_images_per_class:\n",
    "                        file_path = os.path.join(root, filename)\n",
    "                        image = load_img(file_path, target_size=target_size)  # Load with GPU\n",
    "                        image = img_to_array(image)\n",
    "                        image = preprocess_input(image)  # Process the image after loading\n",
    "                        images.append(image)\n",
    "                        labels.append(class_index)\n",
    "                        image_count += 1\n",
    "                    else:\n",
    "                        break  # Stop processing this class if max images reached\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Example usage:\n",
    "directory_path = 'G:/ChineseCheck/'\n",
    "X, y = label_images(directory_path, target_size=(img_width, img_height))\n",
    "\n",
    "# X contains the resized images, and y contains the corresponding labels\n",
    "print(f\"Total Images: {len(X)}\")\n",
    "print(f\"Shape of an Image: {X[0].shape}\")\n",
    "print(f\"Labels: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c800ec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 1800 samples\n",
      "Testing Set: 200 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X and y are the images and labels obtained from the previous code\n",
    "# X, y = label_images(directory_path)\n",
    "\n",
    "# Split the data into 90% training and 10% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Print the sizes of the training and testing sets\n",
    "print(f\"Training Set: {len(X_train)} samples\")\n",
    "print(f\"Testing Set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf57aa8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m692s\u001b[0m 3s/step - accuracy: 0.5362 - loss: 0.9134 - val_accuracy: 0.5300 - val_loss: 0.7072\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m692s\u001b[0m 3s/step - accuracy: 0.5375 - loss: 0.7520 - val_accuracy: 0.4900 - val_loss: 0.7903\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m663s\u001b[0m 3s/step - accuracy: 0.5441 - loss: 0.7259 - val_accuracy: 0.5375 - val_loss: 0.8339\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m677s\u001b[0m 3s/step - accuracy: 0.5274 - loss: 0.7833 - val_accuracy: 0.4750 - val_loss: 1.6466\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m665s\u001b[0m 3s/step - accuracy: 0.5353 - loss: 0.8008 - val_accuracy: 0.4950 - val_loss: 0.9240\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m645s\u001b[0m 3s/step - accuracy: 0.5954 - loss: 0.7022 - val_accuracy: 0.5800 - val_loss: 0.7028\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m630s\u001b[0m 3s/step - accuracy: 0.5648 - loss: 0.6969 - val_accuracy: 0.5200 - val_loss: 0.8558\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m626s\u001b[0m 3s/step - accuracy: 0.5815 - loss: 0.7175 - val_accuracy: 0.5450 - val_loss: 0.7547\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m628s\u001b[0m 3s/step - accuracy: 0.6111 - loss: 0.7162 - val_accuracy: 0.6025 - val_loss: 0.6881\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m625s\u001b[0m 3s/step - accuracy: 0.6103 - loss: 0.7038 - val_accuracy: 0.5900 - val_loss: 0.6806\n",
      "Validation Accuracy for Fold 1: 59.00%\n",
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m622s\u001b[0m 3s/step - accuracy: 0.6179 - loss: 0.6893 - val_accuracy: 0.6125 - val_loss: 0.6444\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m618s\u001b[0m 3s/step - accuracy: 0.6063 - loss: 0.7150 - val_accuracy: 0.6525 - val_loss: 0.6242\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m614s\u001b[0m 3s/step - accuracy: 0.6626 - loss: 0.6378 - val_accuracy: 0.5725 - val_loss: 0.7493\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 3s/step - accuracy: 0.6133 - loss: 0.6926 - val_accuracy: 0.6500 - val_loss: 0.6687\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m616s\u001b[0m 3s/step - accuracy: 0.6624 - loss: 0.6569 - val_accuracy: 0.6700 - val_loss: 0.6020\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m614s\u001b[0m 3s/step - accuracy: 0.6548 - loss: 0.6382 - val_accuracy: 0.5875 - val_loss: 0.8540\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m623s\u001b[0m 3s/step - accuracy: 0.6341 - loss: 0.6862 - val_accuracy: 0.6175 - val_loss: 0.7161\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 3s/step - accuracy: 0.6515 - loss: 0.6413 - val_accuracy: 0.5525 - val_loss: 1.0346\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m618s\u001b[0m 3s/step - accuracy: 0.6649 - loss: 0.6743 - val_accuracy: 0.6350 - val_loss: 0.7579\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 3s/step - accuracy: 0.6882 - loss: 0.5942 - val_accuracy: 0.6150 - val_loss: 0.7078\n",
      "Validation Accuracy for Fold 2: 61.50%\n",
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 3s/step - accuracy: 0.6973 - loss: 0.5986 - val_accuracy: 0.6575 - val_loss: 0.6810\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m626s\u001b[0m 3s/step - accuracy: 0.6880 - loss: 0.6179 - val_accuracy: 0.7400 - val_loss: 0.5358\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m628s\u001b[0m 3s/step - accuracy: 0.7153 - loss: 0.5556 - val_accuracy: 0.5575 - val_loss: 2.0241\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m739s\u001b[0m 3s/step - accuracy: 0.6754 - loss: 0.6325 - val_accuracy: 0.6500 - val_loss: 0.7231\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m638s\u001b[0m 3s/step - accuracy: 0.7544 - loss: 0.5157 - val_accuracy: 0.5000 - val_loss: 12.3914\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m627s\u001b[0m 3s/step - accuracy: 0.5649 - loss: 0.7263 - val_accuracy: 0.5400 - val_loss: 0.7448\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 3s/step - accuracy: 0.5585 - loss: 0.7248 - val_accuracy: 0.5825 - val_loss: 0.6836\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m621s\u001b[0m 3s/step - accuracy: 0.6367 - loss: 0.6708 - val_accuracy: 0.5475 - val_loss: 0.7971\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 3s/step - accuracy: 0.6128 - loss: 0.6724 - val_accuracy: 0.5900 - val_loss: 0.9695\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 3s/step - accuracy: 0.6420 - loss: 0.6529 - val_accuracy: 0.5875 - val_loss: 0.9417\n",
      "Validation Accuracy for Fold 3: 58.75%\n",
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m618s\u001b[0m 3s/step - accuracy: 0.6244 - loss: 0.6868 - val_accuracy: 0.7025 - val_loss: 0.6067\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 3s/step - accuracy: 0.6665 - loss: 0.6327 - val_accuracy: 0.5825 - val_loss: 0.7212\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 3s/step - accuracy: 0.6930 - loss: 0.5847 - val_accuracy: 0.6550 - val_loss: 0.6544\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 3s/step - accuracy: 0.6958 - loss: 0.5974 - val_accuracy: 0.6675 - val_loss: 0.5829\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 3s/step - accuracy: 0.6858 - loss: 0.5715 - val_accuracy: 0.6125 - val_loss: 0.6921\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 3s/step - accuracy: 0.7220 - loss: 0.5503 - val_accuracy: 0.5500 - val_loss: 1.2579\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m616s\u001b[0m 3s/step - accuracy: 0.6679 - loss: 0.6232 - val_accuracy: 0.5450 - val_loss: 1.1705\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 3s/step - accuracy: 0.6754 - loss: 0.6107 - val_accuracy: 0.6400 - val_loss: 0.6497\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m621s\u001b[0m 3s/step - accuracy: 0.7240 - loss: 0.5240 - val_accuracy: 0.6800 - val_loss: 0.5744\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m626s\u001b[0m 3s/step - accuracy: 0.7491 - loss: 0.5034 - val_accuracy: 0.6850 - val_loss: 0.7747\n",
      "Validation Accuracy for Fold 4: 68.50%\n",
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m623s\u001b[0m 3s/step - accuracy: 0.7318 - loss: 0.5277 - val_accuracy: 0.8300 - val_loss: 0.4013\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 3s/step - accuracy: 0.7705 - loss: 0.4851 - val_accuracy: 0.5700 - val_loss: 1.0459\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m638s\u001b[0m 3s/step - accuracy: 0.7768 - loss: 0.4555 - val_accuracy: 0.7925 - val_loss: 0.4234\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m648s\u001b[0m 3s/step - accuracy: 0.8162 - loss: 0.4031 - val_accuracy: 0.7900 - val_loss: 0.4105\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m647s\u001b[0m 3s/step - accuracy: 0.8342 - loss: 0.3778 - val_accuracy: 0.7925 - val_loss: 0.4375\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m858s\u001b[0m 4s/step - accuracy: 0.8136 - loss: 0.3832 - val_accuracy: 0.6150 - val_loss: 0.9015\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1213s\u001b[0m 6s/step - accuracy: 0.8347 - loss: 0.3559 - val_accuracy: 0.7325 - val_loss: 0.6400\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1210s\u001b[0m 6s/step - accuracy: 0.8583 - loss: 0.3281 - val_accuracy: 0.6950 - val_loss: 0.9492\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1199s\u001b[0m 6s/step - accuracy: 0.8866 - loss: 0.2794 - val_accuracy: 0.6300 - val_loss: 1.0701\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1244s\u001b[0m 6s/step - accuracy: 0.9129 - loss: 0.2191 - val_accuracy: 0.7375 - val_loss: 0.6511\n",
      "Validation Accuracy for Fold 5: 73.75%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have three classes (malignant, benign, normal)\n",
    "num_classes = 2\n",
    "img_width, img_height = 224, 224  # Adjust these dimensions based on your data\n",
    "\n",
    "def build_vgg19_model(learn_rate=0.001, momentum=0.9):\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "\n",
    "    # Freeze all layers except the last three\n",
    "    for layer in base_model.layers[:-3]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    optimizer = SGD(learning_rate=learn_rate, momentum=momentum)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build a new VGG19 model\n",
    "vgg19_model = build_vgg19_model()\n",
    "\n",
    "# Number of folds for k-fold cross-validation\n",
    "num_folds = 5\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=110)\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(X, y), 1):\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    y_train_fold_one_hot = to_categorical(y_train_fold, num_classes=num_classes)\n",
    "    y_val_fold_one_hot = to_categorical(y_val_fold, num_classes=num_classes)\n",
    "\n",
    "    # Train the VGG19 model\n",
    "    class_weights = {0: 1.0, 1: 3.0}  # Adjust the weights based on class imbalance\n",
    "\n",
    "    history = vgg19_model.fit(\n",
    "        X_train_fold,\n",
    "        y_train_fold_one_hot,\n",
    "        epochs=10,\n",
    "        validation_data=(X_val_fold, y_val_fold_one_hot),\n",
    "        batch_size=8,\n",
    "        verbose=1,\n",
    "#         class_weight=class_weights\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss, val_acc = vgg19_model.evaluate(X_val_fold, y_val_fold_one_hot, verbose=0)\n",
    "    print(f\"Validation Accuracy for Fold {fold}: {val_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b62a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1252s\u001b[0m 6s/step - accuracy: 0.5325 - loss: 0.9227 - val_accuracy: 0.5675 - val_loss: 0.7821\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m677s\u001b[0m 3s/step - accuracy: 0.6059 - loss: 0.6767 - val_accuracy: 0.5125 - val_loss: 0.9176\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m614s\u001b[0m 3s/step - accuracy: 0.6509 - loss: 0.6185 - val_accuracy: 0.6000 - val_loss: 0.6977\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m610s\u001b[0m 3s/step - accuracy: 0.6691 - loss: 0.6138 - val_accuracy: 0.5725 - val_loss: 0.7736\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m615s\u001b[0m 3s/step - accuracy: 0.7301 - loss: 0.5706 - val_accuracy: 0.5550 - val_loss: 0.7592\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 3s/step - accuracy: 0.7242 - loss: 0.5482 - val_accuracy: 0.5725 - val_loss: 0.7933\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 3s/step - accuracy: 0.7669 - loss: 0.5142 - val_accuracy: 0.5700 - val_loss: 0.7605\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 3s/step - accuracy: 0.7561 - loss: 0.4993 - val_accuracy: 0.5775 - val_loss: 0.8266\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 3s/step - accuracy: 0.7776 - loss: 0.4806 - val_accuracy: 0.5825 - val_loss: 0.8757\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 3s/step - accuracy: 0.7899 - loss: 0.4478 - val_accuracy: 0.5750 - val_loss: 1.1485\n",
      "Validation Accuracy for Fold 1: 57.50%\n",
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m614s\u001b[0m 3s/step - accuracy: 0.7578 - loss: 0.4918 - val_accuracy: 0.8250 - val_loss: 0.4289\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m622s\u001b[0m 3s/step - accuracy: 0.7691 - loss: 0.4819 - val_accuracy: 0.7500 - val_loss: 0.5548\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 3s/step - accuracy: 0.8213 - loss: 0.4235 - val_accuracy: 0.7825 - val_loss: 0.5066\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m608s\u001b[0m 3s/step - accuracy: 0.8366 - loss: 0.3746 - val_accuracy: 0.7575 - val_loss: 0.4950\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m623s\u001b[0m 3s/step - accuracy: 0.8633 - loss: 0.3306 - val_accuracy: 0.7675 - val_loss: 0.5825\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m643s\u001b[0m 3s/step - accuracy: 0.9057 - loss: 0.2466 - val_accuracy: 0.7900 - val_loss: 0.4919\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 3s/step - accuracy: 0.9239 - loss: 0.2042 - val_accuracy: 0.7575 - val_loss: 0.5450\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m614s\u001b[0m 3s/step - accuracy: 0.9179 - loss: 0.2171 - val_accuracy: 0.7700 - val_loss: 0.5168\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 3s/step - accuracy: 0.9338 - loss: 0.2085 - val_accuracy: 0.7725 - val_loss: 0.4757\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 3s/step - accuracy: 0.9257 - loss: 0.1999 - val_accuracy: 0.7575 - val_loss: 0.5124\n",
      "Validation Accuracy for Fold 2: 75.75%\n",
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 3s/step - accuracy: 0.9002 - loss: 0.2468 - val_accuracy: 0.9100 - val_loss: 0.2337\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 3s/step - accuracy: 0.8850 - loss: 0.2770 - val_accuracy: 0.8925 - val_loss: 0.2336\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m627s\u001b[0m 3s/step - accuracy: 0.9334 - loss: 0.2000 - val_accuracy: 0.9200 - val_loss: 0.1999\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m622s\u001b[0m 3s/step - accuracy: 0.9538 - loss: 0.1418 - val_accuracy: 0.9350 - val_loss: 0.1484\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 3s/step - accuracy: 0.9777 - loss: 0.0938 - val_accuracy: 0.9175 - val_loss: 0.1688\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 3s/step - accuracy: 0.9601 - loss: 0.1197 - val_accuracy: 0.9125 - val_loss: 0.2327\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m618s\u001b[0m 3s/step - accuracy: 0.9649 - loss: 0.0993 - val_accuracy: 0.9450 - val_loss: 0.1275\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m615s\u001b[0m 3s/step - accuracy: 0.9768 - loss: 0.0723 - val_accuracy: 0.9375 - val_loss: 0.1604\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 3s/step - accuracy: 0.9821 - loss: 0.0710 - val_accuracy: 0.9400 - val_loss: 0.1899\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 3s/step - accuracy: 0.9758 - loss: 0.0894 - val_accuracy: 0.9225 - val_loss: 0.2298\n",
      "Validation Accuracy for Fold 3: 92.25%\n",
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m614s\u001b[0m 3s/step - accuracy: 0.9555 - loss: 0.1324 - val_accuracy: 0.9800 - val_loss: 0.0553\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 3s/step - accuracy: 0.9575 - loss: 0.1218 - val_accuracy: 0.9950 - val_loss: 0.0530\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 3s/step - accuracy: 0.9678 - loss: 0.1007 - val_accuracy: 0.9825 - val_loss: 0.0568\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m623s\u001b[0m 3s/step - accuracy: 0.9737 - loss: 0.0840 - val_accuracy: 0.9900 - val_loss: 0.0454\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m616s\u001b[0m 3s/step - accuracy: 0.9747 - loss: 0.0847 - val_accuracy: 0.9800 - val_loss: 0.0657\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 3s/step - accuracy: 0.9711 - loss: 0.0777 - val_accuracy: 0.9300 - val_loss: 0.2325\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m610s\u001b[0m 3s/step - accuracy: 0.9578 - loss: 0.1062 - val_accuracy: 0.9825 - val_loss: 0.0503\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m615s\u001b[0m 3s/step - accuracy: 0.9788 - loss: 0.0747 - val_accuracy: 0.9625 - val_loss: 0.1197\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 3s/step - accuracy: 0.9620 - loss: 0.0850 - val_accuracy: 0.9875 - val_loss: 0.0539\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 3s/step - accuracy: 0.9790 - loss: 0.0674 - val_accuracy: 0.9650 - val_loss: 0.0680\n",
      "Validation Accuracy for Fold 4: 96.50%\n",
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 3s/step - accuracy: 0.9616 - loss: 0.1042 - val_accuracy: 0.9400 - val_loss: 0.1557\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 3s/step - accuracy: 0.9600 - loss: 0.1069 - val_accuracy: 0.9925 - val_loss: 0.0351\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 3s/step - accuracy: 0.9679 - loss: 0.0914 - val_accuracy: 0.9975 - val_loss: 0.0218\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m610s\u001b[0m 3s/step - accuracy: 0.9706 - loss: 0.0779 - val_accuracy: 0.9975 - val_loss: 0.0199\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 3s/step - accuracy: 0.9749 - loss: 0.0730 - val_accuracy: 0.9975 - val_loss: 0.0219\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 3s/step - accuracy: 0.9743 - loss: 0.0555 - val_accuracy: 0.9925 - val_loss: 0.0280\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 3s/step - accuracy: 0.9826 - loss: 0.0533 - val_accuracy: 0.9975 - val_loss: 0.0204\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 3s/step - accuracy: 0.9727 - loss: 0.0674 - val_accuracy: 0.9925 - val_loss: 0.0313\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 3s/step - accuracy: 0.9529 - loss: 0.1058 - val_accuracy: 0.9825 - val_loss: 0.0481\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 3s/step - accuracy: 0.9597 - loss: 0.0977 - val_accuracy: 0.9875 - val_loss: 0.0366\n",
      "Validation Accuracy for Fold 5: 98.75%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have three classes (malignant, benign, normal)\n",
    "num_classes = 2\n",
    "img_width, img_height = 224, 224  # Adjust these dimensions based on your data\n",
    "\n",
    "def build_vgg19_model(learn_rate=0.0001, momentum=0.9):\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "\n",
    "    # Freeze all layers except the last three\n",
    "    for layer in base_model.layers[:-3]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    optimizer = SGD(learning_rate=learn_rate, momentum=momentum)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build a new VGG19 model\n",
    "vgg19_model = build_vgg19_model()\n",
    "\n",
    "# Number of folds for k-fold cross-validation\n",
    "num_folds = 5\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=110)\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(X, y), 1):\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    y_train_fold_one_hot = to_categorical(y_train_fold, num_classes=num_classes)\n",
    "    y_val_fold_one_hot = to_categorical(y_val_fold, num_classes=num_classes)\n",
    "\n",
    "    # Train the VGG19 model\n",
    "    class_weights = {0: 1.0, 1: 3.0}  # Adjust the weights based on class imbalance\n",
    "\n",
    "    history = vgg19_model.fit(\n",
    "        X_train_fold,\n",
    "        y_train_fold_one_hot,\n",
    "        epochs=10,\n",
    "        validation_data=(X_val_fold, y_val_fold_one_hot),\n",
    "        batch_size=8,\n",
    "        verbose=1,\n",
    "#         class_weight=class_weights\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss, val_acc = vgg19_model.evaluate(X_val_fold, y_val_fold_one_hot, verbose=0)\n",
    "    print(f\"Validation Accuracy for Fold {fold}: {val_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2df05b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 12s/step\n",
      "Accuracy: 98.75%\n",
      "Confusion Matrix:\n",
      "[[196   4]\n",
      " [  1 199]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       200\n",
      "           1       0.98      0.99      0.99       200\n",
      "\n",
      "    accuracy                           0.99       400\n",
      "   macro avg       0.99      0.99      0.99       400\n",
      "weighted avg       0.99      0.99      0.99       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Function to calculate and print evaluation metrics\n",
    "def evaluate_model_multi_class(model, X, y_true):\n",
    "    # Predictions\n",
    "    y_pred = vgg19_model.predict(X)\n",
    "\n",
    "    # Convert one-hot encoding to class labels\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true_labels, y_pred_labels)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true_labels, y_pred_labels))\n",
    "\n",
    "# Assuming you have trained the model 'inception_model' and loaded the test set 'X_val_fold', 'y_val_fold_one_hot'\n",
    "evaluate_model_multi_class(vgg19_model, X_val_fold, y_val_fold_one_hot)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
