{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ekTxsBC8UhLk4wtVkaknK_aSA65302IJ",
      "authorship_tag": "ABX9TyNd5iu/gFdoBgoPkAASsNyR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhaveshgupta01/BCancerGogo/blob/main/GogoVGG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/drive/MyDrive/MIASUnited"
      ],
      "metadata": {
        "id": "ihNYevrjwD5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJPeHNQqvSNk",
        "outputId": "6aeaab41-610a-463d-8bdc-1746354c1eab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 90 images belonging to 2 classes.\n",
            "Found 22 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "3/3 [==============================] - 114s 36s/step - loss: 0.7109 - accuracy: 0.5778 - val_loss: 0.6846 - val_accuracy: 0.8182\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 81s 29s/step - loss: 0.6118 - accuracy: 0.8222 - val_loss: 0.4788 - val_accuracy: 0.8182\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 83s 32s/step - loss: 0.4374 - accuracy: 0.8333 - val_loss: 0.5189 - val_accuracy: 0.8636\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 81s 29s/step - loss: 0.4414 - accuracy: 0.8778 - val_loss: 0.4240 - val_accuracy: 0.8182\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 79s 30s/step - loss: 0.3550 - accuracy: 0.8667 - val_loss: 0.4299 - val_accuracy: 0.8182\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 76s 26s/step - loss: 0.3692 - accuracy: 0.8444 - val_loss: 0.3812 - val_accuracy: 0.8182\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 82s 29s/step - loss: 0.3407 - accuracy: 0.8556 - val_loss: 0.3599 - val_accuracy: 0.8182\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 81s 31s/step - loss: 0.3618 - accuracy: 0.8889 - val_loss: 0.4291 - val_accuracy: 0.8636\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 82s 32s/step - loss: 0.3366 - accuracy: 0.8778 - val_loss: 0.3584 - val_accuracy: 0.8182\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 81s 29s/step - loss: 0.3317 - accuracy: 0.8667 - val_loss: 0.3351 - val_accuracy: 0.8636\n",
            "Found 112 images belonging to 2 classes.\n",
            "4/4 [==============================] - 74s 18s/step - loss: 0.2896 - accuracy: 0.8929\n",
            "Test Accuracy: 0.8928571343421936\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define paths to your dataset\n",
        "data_dir = '/content/drive/MyDrive/MIASUnited'\n",
        "\n",
        "# Image dimensions and batch size\n",
        "img_width, img_height = 224, 224\n",
        "batch_size = 32\n",
        "\n",
        "# Data augmentation for training set\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2  # Set the validation split\n",
        ")\n",
        "\n",
        "# Load and augment data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='training'  # Specify training subset\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='validation'  # Specify validation subset\n",
        ")\n",
        "\n",
        "# Define the base model (VGG16 without top layers)\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "\n",
        "# Freeze the layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Custom model for breast cancer detection\n",
        "model = models.Sequential()\n",
        "model.add(base_model)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), #Best Hyperparameters:  {'learning_rate': 0.001}\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,  # Adjust the number of epochs\n",
        "    validation_data=validation_generator,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=False  # Ensure no shuffling for correct evaluation\n",
        ")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f'Test Accuracy: {test_acc}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Define paths to your dataset\n",
        "data_dir = '/content/drive/MyDrive/MIASUnits'\n",
        "\n",
        "# Image dimensions and batch size\n",
        "img_width, img_height = 224, 224\n",
        "batch_size = 32\n",
        "\n",
        "# Data augmentation for training set\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        ")\n",
        "\n",
        "# Load and augment data\n",
        "datagen = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Extract labels for StratifiedKFold\n",
        "labels = datagen.labels\n",
        "\n",
        "# Convert labels to integers (if needed)\n",
        "labels = np.array(labels, dtype=int)\n",
        "\n",
        "# Define the base model (VGG16 without top layers)\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "\n",
        "# Freeze the layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Custom model for breast cancer detection\n",
        "def build_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(base_model)\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# K-Fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold = 1\n",
        "final_accuracy = []\n",
        "\n",
        "for train_index, val_index in skf.split(datagen.filenames, labels):\n",
        "    print(f\"Fold {fold}/5\")\n",
        "\n",
        "    # Split the data\n",
        "    train_data = np.array(datagen.filenames)[train_index]\n",
        "    val_data = np.array(datagen.filenames)[val_index]\n",
        "\n",
        "    # Use the training data for both training and validation generators\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        subset='training',  # Use train indices\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    validation_generator = train_datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        subset='training',  # Use train indices for validation too\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model()\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=10,  # Adjust the number of epochs\n",
        "        validation_data=validation_generator,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    test_loss, test_acc = model.evaluate(validation_generator)\n",
        "    final_accuracy.append(test_acc)\n",
        "\n",
        "    fold += 1\n",
        "\n",
        "# Print final results\n",
        "print(\"Final Accuracy:\", np.mean(final_accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-ldMFGX0tH6",
        "outputId": "7f3e8342-3b63-4b82-a0ec-8e01236d8e37"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 112 images belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Fold 1/5\n",
            "Found 112 images belonging to 2 classes.\n",
            "Found 112 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 166s 45s/step - loss: 0.7202 - accuracy: 0.5982 - val_loss: 0.6861 - val_accuracy: 0.5625\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 149s 43s/step - loss: 0.7042 - accuracy: 0.5268 - val_loss: 0.6634 - val_accuracy: 0.5536\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 159s 50s/step - loss: 0.6575 - accuracy: 0.6161 - val_loss: 0.6604 - val_accuracy: 0.6071\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 156s 48s/step - loss: 0.6644 - accuracy: 0.5893 - val_loss: 0.6394 - val_accuracy: 0.5893\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 152s 42s/step - loss: 0.6483 - accuracy: 0.6250 - val_loss: 0.6914 - val_accuracy: 0.4911\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 156s 45s/step - loss: 0.6852 - accuracy: 0.5357 - val_loss: 0.6570 - val_accuracy: 0.5714\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 156s 45s/step - loss: 0.6486 - accuracy: 0.5804 - val_loss: 0.5953 - val_accuracy: 0.7857\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 146s 42s/step - loss: 0.6173 - accuracy: 0.7232 - val_loss: 0.6175 - val_accuracy: 0.6518\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 158s 45s/step - loss: 0.6223 - accuracy: 0.6607 - val_loss: 0.6061 - val_accuracy: 0.6696\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 149s 43s/step - loss: 0.6007 - accuracy: 0.7232 - val_loss: 0.6008 - val_accuracy: 0.6607\n",
            "4/4 [==============================] - 75s 17s/step - loss: 0.5975 - accuracy: 0.7054\n",
            "Fold 2/5\n",
            "Found 112 images belonging to 2 classes.\n",
            "Found 112 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 151s 46s/step - loss: 1.2535 - accuracy: 0.5357 - val_loss: 0.8324 - val_accuracy: 0.4554\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 159s 45s/step - loss: 0.9128 - accuracy: 0.4554 - val_loss: 0.7271 - val_accuracy: 0.4643\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 156s 44s/step - loss: 0.7190 - accuracy: 0.5536 - val_loss: 0.7933 - val_accuracy: 0.5446\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 155s 48s/step - loss: 0.7625 - accuracy: 0.5536 - val_loss: 0.6771 - val_accuracy: 0.5804\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 152s 43s/step - loss: 0.6806 - accuracy: 0.5536 - val_loss: 0.6775 - val_accuracy: 0.5446\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 155s 44s/step - loss: 0.6590 - accuracy: 0.6071 - val_loss: 0.6468 - val_accuracy: 0.6786\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 148s 42s/step - loss: 0.6701 - accuracy: 0.5625 - val_loss: 0.6407 - val_accuracy: 0.6518\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 157s 49s/step - loss: 0.6381 - accuracy: 0.6607 - val_loss: 0.6365 - val_accuracy: 0.6071\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 155s 48s/step - loss: 0.6439 - accuracy: 0.5804 - val_loss: 0.6242 - val_accuracy: 0.6607\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 156s 44s/step - loss: 0.6404 - accuracy: 0.6696 - val_loss: 0.6175 - val_accuracy: 0.6786\n",
            "4/4 [==============================] - 73s 17s/step - loss: 0.6394 - accuracy: 0.6339\n",
            "Fold 3/5\n",
            "Found 112 images belonging to 2 classes.\n",
            "Found 112 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 162s 49s/step - loss: 1.0954 - accuracy: 0.3929 - val_loss: 0.8644 - val_accuracy: 0.5446\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 156s 45s/step - loss: 0.8968 - accuracy: 0.5446 - val_loss: 0.6958 - val_accuracy: 0.5446\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 160s 44s/step - loss: 0.7218 - accuracy: 0.5179 - val_loss: 0.7992 - val_accuracy: 0.4554\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 156s 45s/step - loss: 0.7335 - accuracy: 0.5000 - val_loss: 0.6780 - val_accuracy: 0.5446\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 156s 45s/step - loss: 0.7477 - accuracy: 0.5446 - val_loss: 0.6991 - val_accuracy: 0.5446\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 159s 46s/step - loss: 0.6535 - accuracy: 0.6429 - val_loss: 0.6870 - val_accuracy: 0.5357\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 156s 48s/step - loss: 0.7022 - accuracy: 0.5536 - val_loss: 0.6604 - val_accuracy: 0.5804\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 156s 48s/step - loss: 0.6446 - accuracy: 0.5982 - val_loss: 0.6390 - val_accuracy: 0.5804\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 156s 48s/step - loss: 0.6496 - accuracy: 0.5625 - val_loss: 0.6196 - val_accuracy: 0.6250\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 155s 48s/step - loss: 0.6188 - accuracy: 0.6875 - val_loss: 0.6297 - val_accuracy: 0.7321\n",
            "4/4 [==============================] - 74s 17s/step - loss: 0.6247 - accuracy: 0.7232\n",
            "Fold 4/5\n",
            "Found 112 images belonging to 2 classes.\n",
            "Found 112 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 153s 43s/step - loss: 0.8591 - accuracy: 0.4554 - val_loss: 0.8194 - val_accuracy: 0.4554\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 146s 41s/step - loss: 0.7771 - accuracy: 0.5268 - val_loss: 0.6850 - val_accuracy: 0.5536\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 148s 42s/step - loss: 0.7370 - accuracy: 0.5536 - val_loss: 0.6642 - val_accuracy: 0.5982\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 156s 45s/step - loss: 0.6603 - accuracy: 0.5893 - val_loss: 0.6877 - val_accuracy: 0.5089\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 155s 44s/step - loss: 0.7102 - accuracy: 0.5000 - val_loss: 0.6395 - val_accuracy: 0.6071\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 152s 47s/step - loss: 0.6688 - accuracy: 0.6071 - val_loss: 0.6426 - val_accuracy: 0.6607\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 157s 45s/step - loss: 0.6398 - accuracy: 0.6518 - val_loss: 0.6389 - val_accuracy: 0.5625\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 155s 44s/step - loss: 0.6337 - accuracy: 0.5714 - val_loss: 0.6268 - val_accuracy: 0.6696\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 151s 46s/step - loss: 0.6681 - accuracy: 0.5536 - val_loss: 0.6382 - val_accuracy: 0.6696\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 156s 45s/step - loss: 0.6613 - accuracy: 0.5714 - val_loss: 0.6371 - val_accuracy: 0.5536\n",
            "4/4 [==============================] - 76s 18s/step - loss: 0.6359 - accuracy: 0.5804\n",
            "Fold 5/5\n",
            "Found 112 images belonging to 2 classes.\n",
            "Found 112 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 158s 44s/step - loss: 0.8445 - accuracy: 0.4554 - val_loss: 0.8296 - val_accuracy: 0.4554\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 156s 48s/step - loss: 0.7597 - accuracy: 0.4464 - val_loss: 0.7551 - val_accuracy: 0.5446\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 155s 44s/step - loss: 0.7461 - accuracy: 0.5446 - val_loss: 0.6817 - val_accuracy: 0.5089\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 156s 48s/step - loss: 0.7156 - accuracy: 0.4732 - val_loss: 0.6663 - val_accuracy: 0.6071\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 157s 45s/step - loss: 0.6604 - accuracy: 0.5536 - val_loss: 0.6702 - val_accuracy: 0.5446\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 159s 46s/step - loss: 0.6546 - accuracy: 0.5625 - val_loss: 0.6517 - val_accuracy: 0.6607\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 155s 44s/step - loss: 0.6972 - accuracy: 0.5000 - val_loss: 0.6445 - val_accuracy: 0.6786\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 160s 46s/step - loss: 0.6196 - accuracy: 0.6607 - val_loss: 0.6416 - val_accuracy: 0.5625\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 156s 45s/step - loss: 0.6438 - accuracy: 0.5714 - val_loss: 0.6027 - val_accuracy: 0.7679\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 155s 48s/step - loss: 0.6129 - accuracy: 0.7411 - val_loss: 0.6246 - val_accuracy: 0.6964\n",
            "4/4 [==============================] - 73s 17s/step - loss: 0.5957 - accuracy: 0.7589\n",
            "Final Accuracy: 0.6803571462631226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Define paths to your dataset\n",
        "data_dir = '/content/drive/MyDrive/MIASUnited'\n",
        "\n",
        "# Image dimensions and batch size\n",
        "img_width, img_height = 224, 224\n",
        "batch_size = 32\n",
        "\n",
        "# Data augmentation for training set\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        ")\n",
        "\n",
        "# Load and augment data\n",
        "datagen = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Extract labels for StratifiedKFold\n",
        "labels = datagen.labels\n",
        "\n",
        "# Convert labels to integers (if needed)\n",
        "labels = np.array(labels, dtype=int)\n",
        "\n",
        "# Define the base model (VGG16 without top layers)\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "\n",
        "# Freeze the layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Custom model for breast cancer detection\n",
        "def build_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(base_model)\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# K-Fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold = 1\n",
        "final_accuracy = []\n",
        "\n",
        "for train_index, val_index in skf.split(datagen.filenames, labels):\n",
        "    print(f\"Fold {fold}/5\")\n",
        "\n",
        "    # Split the data\n",
        "    train_data = np.array(datagen.filenames)[train_index]\n",
        "    val_data = np.array(datagen.filenames)[val_index]\n",
        "\n",
        "    # Use the training data for both training and validation generators\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        subset='training',  # Use train indices\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    validation_generator = train_datagen.flow_from_directory(\n",
        "        data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary',\n",
        "        subset='training',  # Use train indices for validation too\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    # Build the model\n",
        "    model = build_model()\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=10,  # Adjust the number of epochs\n",
        "        validation_data=validation_generator,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    test_loss, test_acc = model.evaluate(validation_generator)\n",
        "    final_accuracy.append(test_acc)\n",
        "\n",
        "    fold += 1\n",
        "\n",
        "# Print final results\n",
        "print(\"Final Accuracy:\", np.mean(final_accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pzd3NbYoEMzp",
        "outputId": "5810aafb-33bb-44d9-96d4-e4c276a17186"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 112 images belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Fold 1/5\n",
            "Found 112 images belonging to 2 classes.\n",
            "Found 112 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 97s 28s/step - loss: 0.6697 - accuracy: 0.8214 - val_loss: 0.5188 - val_accuracy: 0.8214\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 90s 25s/step - loss: 0.5041 - accuracy: 0.8304 - val_loss: 0.5074 - val_accuracy: 0.8214\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 90s 26s/step - loss: 0.4321 - accuracy: 0.8304 - val_loss: 0.3896 - val_accuracy: 0.8304\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 93s 27s/step - loss: 0.4044 - accuracy: 0.8304 - val_loss: 0.3855 - val_accuracy: 0.8393\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 126s 38s/step - loss: 0.3819 - accuracy: 0.8482 - val_loss: 0.3553 - val_accuracy: 0.8839\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 88s 25s/step - loss: 0.3646 - accuracy: 0.8839 - val_loss: 0.3469 - val_accuracy: 0.8750\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 90s 28s/step - loss: 0.3305 - accuracy: 0.8750 - val_loss: 0.3435 - val_accuracy: 0.8661\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 89s 27s/step - loss: 0.3084 - accuracy: 0.9018 - val_loss: 0.3159 - val_accuracy: 0.8929\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 89s 25s/step - loss: 0.3197 - accuracy: 0.8750 - val_loss: 0.3015 - val_accuracy: 0.9018\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 89s 25s/step - loss: 0.3037 - accuracy: 0.9107 - val_loss: 0.2863 - val_accuracy: 0.8839\n",
            "4/4 [==============================] - 44s 10s/step - loss: 0.3229 - accuracy: 0.8661\n",
            "Fold 2/5\n",
            "Found 112 images belonging to 2 classes.\n",
            "Found 112 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 128s 38s/step - loss: 0.8271 - accuracy: 0.6429 - val_loss: 0.7670 - val_accuracy: 0.8214\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 90s 26s/step - loss: 0.7100 - accuracy: 0.8214 - val_loss: 0.5274 - val_accuracy: 0.8214\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 126s 38s/step - loss: 0.4391 - accuracy: 0.8214 - val_loss: 0.4744 - val_accuracy: 0.8571\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 89s 25s/step - loss: 0.4795 - accuracy: 0.8750 - val_loss: 0.4036 - val_accuracy: 0.8661\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 89s 25s/step - loss: 0.3885 - accuracy: 0.8304 - val_loss: 0.4009 - val_accuracy: 0.8214\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 90s 25s/step - loss: 0.4267 - accuracy: 0.8214 - val_loss: 0.3780 - val_accuracy: 0.8214\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 126s 40s/step - loss: 0.3353 - accuracy: 0.8571 - val_loss: 0.3492 - val_accuracy: 0.8571\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 88s 25s/step - loss: 0.3422 - accuracy: 0.8929 - val_loss: 0.3420 - val_accuracy: 0.9018\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 90s 26s/step - loss: 0.3361 - accuracy: 0.8929 - val_loss: 0.3106 - val_accuracy: 0.8750\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 89s 25s/step - loss: 0.3631 - accuracy: 0.8571 - val_loss: 0.3445 - val_accuracy: 0.8571\n",
            "4/4 [==============================] - 44s 10s/step - loss: 0.3430 - accuracy: 0.8482\n",
            "Fold 3/5\n",
            "Found 112 images belonging to 2 classes.\n",
            "Found 112 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 90s 25s/step - loss: 0.6273 - accuracy: 0.7589 - val_loss: 0.5100 - val_accuracy: 0.8214\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 90s 25s/step - loss: 0.5070 - accuracy: 0.8304 - val_loss: 0.4688 - val_accuracy: 0.8571\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 126s 38s/step - loss: 0.4149 - accuracy: 0.8661 - val_loss: 0.3832 - val_accuracy: 0.8214\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 88s 25s/step - loss: 0.3780 - accuracy: 0.8393 - val_loss: 0.3579 - val_accuracy: 0.8214\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 90s 25s/step - loss: 0.3721 - accuracy: 0.8571 - val_loss: 0.3304 - val_accuracy: 0.9018\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 126s 38s/step - loss: 0.3307 - accuracy: 0.8839 - val_loss: 0.3374 - val_accuracy: 0.8929\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 88s 25s/step - loss: 0.3341 - accuracy: 0.8571 - val_loss: 0.3137 - val_accuracy: 0.9018\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 88s 27s/step - loss: 0.3269 - accuracy: 0.8750 - val_loss: 0.3114 - val_accuracy: 0.8929\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 89s 27s/step - loss: 0.3112 - accuracy: 0.8839 - val_loss: 0.2914 - val_accuracy: 0.9286\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 89s 25s/step - loss: 0.3019 - accuracy: 0.8750 - val_loss: 0.3241 - val_accuracy: 0.8839\n",
            "4/4 [==============================] - 44s 10s/step - loss: 0.3221 - accuracy: 0.8571\n",
            "Fold 4/5\n",
            "Found 112 images belonging to 2 classes.\n",
            "Found 112 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 90s 25s/step - loss: 0.4383 - accuracy: 0.8214 - val_loss: 0.4941 - val_accuracy: 0.8214\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 126s 40s/step - loss: 0.6374 - accuracy: 0.8214 - val_loss: 0.3810 - val_accuracy: 0.8214\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 125s 40s/step - loss: 0.4225 - accuracy: 0.8482 - val_loss: 0.4363 - val_accuracy: 0.8482\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 88s 27s/step - loss: 0.3750 - accuracy: 0.8571 - val_loss: 0.3994 - val_accuracy: 0.8482\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 88s 25s/step - loss: 0.4005 - accuracy: 0.8482 - val_loss: 0.3238 - val_accuracy: 0.8839\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 89s 25s/step - loss: 0.3345 - accuracy: 0.8839 - val_loss: 0.3464 - val_accuracy: 0.8929\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 126s 38s/step - loss: 0.3269 - accuracy: 0.8839 - val_loss: 0.3029 - val_accuracy: 0.9107\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 126s 38s/step - loss: 0.3303 - accuracy: 0.8929 - val_loss: 0.2889 - val_accuracy: 0.9018\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 88s 25s/step - loss: 0.2767 - accuracy: 0.9018 - val_loss: 0.2987 - val_accuracy: 0.9107\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 89s 25s/step - loss: 0.2798 - accuracy: 0.9107 - val_loss: 0.2936 - val_accuracy: 0.9286\n",
            "4/4 [==============================] - 44s 10s/step - loss: 0.2674 - accuracy: 0.9286\n",
            "Fold 5/5\n",
            "Found 112 images belonging to 2 classes.\n",
            "Found 112 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 127s 38s/step - loss: 0.6240 - accuracy: 0.8214 - val_loss: 0.5423 - val_accuracy: 0.8214\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 128s 38s/step - loss: 0.4651 - accuracy: 0.8214 - val_loss: 0.4641 - val_accuracy: 0.8393\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 126s 38s/step - loss: 0.4391 - accuracy: 0.8571 - val_loss: 0.4106 - val_accuracy: 0.8214\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 125s 40s/step - loss: 0.3780 - accuracy: 0.8304 - val_loss: 0.3751 - val_accuracy: 0.8571\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 88s 27s/step - loss: 0.3451 - accuracy: 0.8482 - val_loss: 0.3337 - val_accuracy: 0.8750\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 126s 38s/step - loss: 0.4006 - accuracy: 0.8839 - val_loss: 0.3359 - val_accuracy: 0.8661\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 88s 27s/step - loss: 0.3782 - accuracy: 0.8482 - val_loss: 0.3433 - val_accuracy: 0.8482\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 89s 25s/step - loss: 0.3541 - accuracy: 0.8571 - val_loss: 0.2962 - val_accuracy: 0.9107\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 126s 38s/step - loss: 0.3153 - accuracy: 0.9196 - val_loss: 0.3082 - val_accuracy: 0.9107\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 89s 25s/step - loss: 0.2920 - accuracy: 0.9107 - val_loss: 0.2916 - val_accuracy: 0.9018\n",
            "4/4 [==============================] - 44s 10s/step - loss: 0.2871 - accuracy: 0.8929\n",
            "Final Accuracy: 0.8785714149475098\n"
          ]
        }
      ]
    }
  ]
}