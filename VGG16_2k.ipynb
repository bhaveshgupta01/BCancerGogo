{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9534c93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images: 2000\n",
      "Shape of an Image: (224, 224, 3)\n",
      "Labels: [0 0 0 ... 1 1 1]\n",
      "Training Set: 1800 samples\n",
      "Testing Set: 200 samples\n",
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m735s\u001b[0m 4s/step - accuracy: 0.5242 - loss: 0.9518 - val_accuracy: 0.5025 - val_loss: 1.1222\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 4s/step - accuracy: 0.6265 - loss: 0.6516 - val_accuracy: 0.5625 - val_loss: 0.8603\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m722s\u001b[0m 4s/step - accuracy: 0.6522 - loss: 0.6377 - val_accuracy: 0.5850 - val_loss: 0.7003\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m722s\u001b[0m 4s/step - accuracy: 0.6970 - loss: 0.5690 - val_accuracy: 0.5400 - val_loss: 0.8029\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m723s\u001b[0m 4s/step - accuracy: 0.7221 - loss: 0.5430 - val_accuracy: 0.5525 - val_loss: 0.8969\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m723s\u001b[0m 4s/step - accuracy: 0.7321 - loss: 0.5356 - val_accuracy: 0.5775 - val_loss: 0.8629\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 3s/step - accuracy: 0.7695 - loss: 0.4753 - val_accuracy: 0.5550 - val_loss: 0.7910\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m505s\u001b[0m 3s/step - accuracy: 0.7717 - loss: 0.4632 - val_accuracy: 0.5925 - val_loss: 0.8950\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m493s\u001b[0m 2s/step - accuracy: 0.7898 - loss: 0.4442 - val_accuracy: 0.5925 - val_loss: 0.8524\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 2s/step - accuracy: 0.8287 - loss: 0.3741 - val_accuracy: 0.6275 - val_loss: 0.8172\n",
      "Validation Accuracy for Fold 1: 62.75%\n",
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 2s/step - accuracy: 0.8000 - loss: 0.4492 - val_accuracy: 0.8925 - val_loss: 0.3125\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 2s/step - accuracy: 0.8293 - loss: 0.3918 - val_accuracy: 0.7925 - val_loss: 0.4297\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 2s/step - accuracy: 0.8773 - loss: 0.3109 - val_accuracy: 0.8150 - val_loss: 0.4454\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 2s/step - accuracy: 0.8970 - loss: 0.2724 - val_accuracy: 0.8300 - val_loss: 0.3862\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 2s/step - accuracy: 0.9406 - loss: 0.1916 - val_accuracy: 0.8225 - val_loss: 0.4110\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 2s/step - accuracy: 0.9286 - loss: 0.1811 - val_accuracy: 0.7950 - val_loss: 0.4886\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 2s/step - accuracy: 0.9135 - loss: 0.2191 - val_accuracy: 0.7950 - val_loss: 0.4869\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 2s/step - accuracy: 0.9504 - loss: 0.1431 - val_accuracy: 0.7625 - val_loss: 0.6334\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 2s/step - accuracy: 0.9524 - loss: 0.1475 - val_accuracy: 0.7925 - val_loss: 0.4969\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 2s/step - accuracy: 0.9649 - loss: 0.1069 - val_accuracy: 0.8000 - val_loss: 0.5417\n",
      "Validation Accuracy for Fold 2: 80.00%\n",
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 3s/step - accuracy: 0.8880 - loss: 0.2914 - val_accuracy: 0.9350 - val_loss: 0.1824\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 3s/step - accuracy: 0.9358 - loss: 0.1896 - val_accuracy: 0.8550 - val_loss: 0.3483\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 2s/step - accuracy: 0.9488 - loss: 0.1570 - val_accuracy: 0.8800 - val_loss: 0.2756\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 2s/step - accuracy: 0.9558 - loss: 0.1268 - val_accuracy: 0.9325 - val_loss: 0.1687\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 2s/step - accuracy: 0.9654 - loss: 0.1124 - val_accuracy: 0.9400 - val_loss: 0.1810\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 2s/step - accuracy: 0.9541 - loss: 0.1161 - val_accuracy: 0.9875 - val_loss: 0.0827\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 2s/step - accuracy: 0.9734 - loss: 0.0858 - val_accuracy: 0.9425 - val_loss: 0.1476\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 2s/step - accuracy: 0.9735 - loss: 0.0865 - val_accuracy: 0.9575 - val_loss: 0.1342\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 2s/step - accuracy: 0.9712 - loss: 0.0848 - val_accuracy: 0.9400 - val_loss: 0.1443\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 2s/step - accuracy: 0.9511 - loss: 0.1361 - val_accuracy: 0.9150 - val_loss: 0.1940\n",
      "Validation Accuracy for Fold 3: 91.50%\n",
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 2s/step - accuracy: 0.9454 - loss: 0.1477 - val_accuracy: 0.9900 - val_loss: 0.0247\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 2s/step - accuracy: 0.9637 - loss: 0.0985 - val_accuracy: 0.9975 - val_loss: 0.0247\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 2s/step - accuracy: 0.9719 - loss: 0.0832 - val_accuracy: 0.9900 - val_loss: 0.0484\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 2s/step - accuracy: 0.9679 - loss: 0.0978 - val_accuracy: 0.9950 - val_loss: 0.0262\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 2s/step - accuracy: 0.9826 - loss: 0.0604 - val_accuracy: 0.9900 - val_loss: 0.0382\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 2s/step - accuracy: 0.9693 - loss: 0.0964 - val_accuracy: 0.9950 - val_loss: 0.0377\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m488s\u001b[0m 2s/step - accuracy: 0.9924 - loss: 0.0528 - val_accuracy: 0.9925 - val_loss: 0.0294\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 2s/step - accuracy: 0.9800 - loss: 0.0602 - val_accuracy: 0.9875 - val_loss: 0.0430\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 2s/step - accuracy: 0.9856 - loss: 0.0511 - val_accuracy: 1.0000 - val_loss: 0.0235\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 2s/step - accuracy: 0.9837 - loss: 0.0516 - val_accuracy: 0.9900 - val_loss: 0.0332\n",
      "Validation Accuracy for Fold 4: 99.00%\n",
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 2s/step - accuracy: 0.9824 - loss: 0.0558 - val_accuracy: 0.9950 - val_loss: 0.0303\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m491s\u001b[0m 2s/step - accuracy: 0.9705 - loss: 0.0720 - val_accuracy: 0.9850 - val_loss: 0.0607\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 2s/step - accuracy: 0.9693 - loss: 0.0971 - val_accuracy: 0.9975 - val_loss: 0.0147\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m488s\u001b[0m 2s/step - accuracy: 0.9847 - loss: 0.0566 - val_accuracy: 0.9850 - val_loss: 0.0530\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 2s/step - accuracy: 0.9816 - loss: 0.0670 - val_accuracy: 0.9850 - val_loss: 0.0567\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 2s/step - accuracy: 0.9753 - loss: 0.0688 - val_accuracy: 0.9900 - val_loss: 0.0334\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m493s\u001b[0m 2s/step - accuracy: 0.9730 - loss: 0.0664 - val_accuracy: 0.9800 - val_loss: 0.0525\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 2s/step - accuracy: 0.9835 - loss: 0.0604 - val_accuracy: 0.9975 - val_loss: 0.0212\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 2s/step - accuracy: 0.9819 - loss: 0.0496 - val_accuracy: 0.9900 - val_loss: 0.0310\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 2s/step - accuracy: 0.9924 - loss: 0.0266 - val_accuracy: 0.9950 - val_loss: 0.0337\n",
      "Validation Accuracy for Fold 5: 99.50%\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 7s/step\n",
      "Accuracy: 99.50%\n",
      "Confusion Matrix:\n",
      "[[200   0]\n",
      " [  2 198]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       200\n",
      "           1       1.00      0.99      0.99       200\n",
      "\n",
      "    accuracy                           0.99       400\n",
      "   macro avg       1.00      0.99      0.99       400\n",
      "weighted avg       1.00      0.99      0.99       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Image dimensions and batch size\n",
    "img_width, img_height = 224, 224\n",
    "batch_size = 8\n",
    "\n",
    "# ... (GPU availability and memory growth code from previous response)\n",
    "def label_images(directory, target_size=(img_width, img_height), max_images_per_class=1000):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_labels = {'Malignant': 0, 'Benign': 1}\n",
    "\n",
    "    for class_label, class_index in class_labels.items():\n",
    "        class_path = os.path.join(directory, class_label)\n",
    "        image_count = 0\n",
    "        for root, _, files in os.walk(class_path):\n",
    "            for filename in files:\n",
    "                if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                    if image_count < max_images_per_class:\n",
    "                        file_path = os.path.join(root, filename)\n",
    "                        image = load_img(file_path, target_size=target_size)  # Load with GPU\n",
    "                        image = img_to_array(image)\n",
    "                        image = preprocess_input(image)  # Process the image after loading\n",
    "                        images.append(image)\n",
    "                        labels.append(class_index)\n",
    "                        image_count += 1\n",
    "                    else:\n",
    "                        break  # Stop processing this class if max images reached\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Example usage:\n",
    "directory_path = 'G:/ChineseCheck/'\n",
    "X, y = label_images(directory_path, target_size=(img_width, img_height))\n",
    "\n",
    "# X contains the resized images, and y contains the corresponding labels\n",
    "print(f\"Total Images: {len(X)}\")\n",
    "print(f\"Shape of an Image: {X[0].shape}\")\n",
    "print(f\"Labels: {y}\")\n",
    "\n",
    "# Split the data into 90% training and 10% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Print the sizes of the training and testing sets\n",
    "print(f\"Training Set: {len(X_train)} samples\")\n",
    "print(f\"Testing Set: {len(X_test)} samples\")\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming you have three classes (malignant, benign, normal)\n",
    "num_classes = 2\n",
    "img_width, img_height = 224, 224  # Adjust these dimensions based on your data\n",
    "\n",
    "def build_vgg16_model(learn_rate=0.0001, momentum=0.9):\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "\n",
    "    # Freeze all layers except the last three\n",
    "    for layer in base_model.layers[:-3]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    optimizer = SGD(learning_rate=learn_rate, momentum=momentum)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build a new VGG16 model\n",
    "vgg16_model = build_vgg16_model()\n",
    "\n",
    "# Number of folds for k-fold cross-validation\n",
    "num_folds = 5\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=110)\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(X, y), 1):\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    y_train_fold_one_hot = to_categorical(y_train_fold, num_classes=num_classes)\n",
    "    y_val_fold_one_hot = to_categorical(y_val_fold, num_classes=num_classes)\n",
    "\n",
    "    # Train the VGG16 model\n",
    "    class_weights = {0: 1.0, 1: 3.0}  # Adjust the weights based on class imbalance\n",
    "\n",
    "    history = vgg16_model.fit(\n",
    "        X_train_fold,\n",
    "        y_train_fold_one_hot,\n",
    "        epochs=10,\n",
    "        validation_data=(X_val_fold, y_val_fold_one_hot),\n",
    "        batch_size=8,\n",
    "        verbose=1,\n",
    "#         class_weight=class_weights\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss, val_acc = vgg16_model.evaluate(X_val_fold, y_val_fold_one_hot, verbose=0)\n",
    "    print(f\"Validation Accuracy for Fold {fold}: {val_acc * 100:.2f}%\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Function to calculate and print evaluation metrics for multi-class classification\n",
    "def evaluate_model_multi_class(model, X, y_true):\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Convert one-hot encoding to class labels\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true_labels, y_pred_labels)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true_labels, y_pred_labels))\n",
    "\n",
    "    # Assuming you have trained the model 'vgg16_model' and loaded the test set 'X_val_fold', 'y_val_fold_one_hot'\n",
    "evaluate_model_multi_class(vgg16_model, X_val_fold, y_val_fold_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb62e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
