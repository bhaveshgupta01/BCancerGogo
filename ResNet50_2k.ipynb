{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb94f2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images: 2000\n",
      "Shape of an Image: (224, 224, 3)\n",
      "Labels: [0 0 0 ... 1 1 1]\n",
      "Training Set: 1800 samples\n",
      "Testing Set: 200 samples\n",
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 904ms/step - accuracy: 0.5270 - loss: 1.1030 - val_accuracy: 0.5750 - val_loss: 0.8105\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 898ms/step - accuracy: 0.6898 - loss: 0.5672 - val_accuracy: 0.5925 - val_loss: 0.7366\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 897ms/step - accuracy: 0.7635 - loss: 0.4948 - val_accuracy: 0.6025 - val_loss: 0.7143\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 897ms/step - accuracy: 0.8052 - loss: 0.4268 - val_accuracy: 0.6025 - val_loss: 0.8422\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 892ms/step - accuracy: 0.8525 - loss: 0.3663 - val_accuracy: 0.5825 - val_loss: 0.8076\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 890ms/step - accuracy: 0.8747 - loss: 0.3314 - val_accuracy: 0.5775 - val_loss: 0.8396\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 887ms/step - accuracy: 0.8761 - loss: 0.3030 - val_accuracy: 0.6000 - val_loss: 0.7876\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 878ms/step - accuracy: 0.9233 - loss: 0.2389 - val_accuracy: 0.5650 - val_loss: 0.9268\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 877ms/step - accuracy: 0.9106 - loss: 0.2543 - val_accuracy: 0.6025 - val_loss: 0.8587\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 880ms/step - accuracy: 0.9270 - loss: 0.2176 - val_accuracy: 0.5850 - val_loss: 1.0011\n",
      "Validation Accuracy for Fold 1: 58.50%\n",
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 878ms/step - accuracy: 0.8263 - loss: 0.4337 - val_accuracy: 0.8550 - val_loss: 0.2858\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 879ms/step - accuracy: 0.8994 - loss: 0.3025 - val_accuracy: 0.8650 - val_loss: 0.2992\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 880ms/step - accuracy: 0.9081 - loss: 0.2582 - val_accuracy: 0.8400 - val_loss: 0.3384\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 880ms/step - accuracy: 0.9062 - loss: 0.2586 - val_accuracy: 0.9350 - val_loss: 0.2086\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 877ms/step - accuracy: 0.9199 - loss: 0.2312 - val_accuracy: 0.8900 - val_loss: 0.2659\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 879ms/step - accuracy: 0.9431 - loss: 0.1929 - val_accuracy: 0.9100 - val_loss: 0.2404\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 875ms/step - accuracy: 0.9374 - loss: 0.2042 - val_accuracy: 0.8700 - val_loss: 0.3100\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 879ms/step - accuracy: 0.9484 - loss: 0.1666 - val_accuracy: 0.8850 - val_loss: 0.2630\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 878ms/step - accuracy: 0.9471 - loss: 0.1654 - val_accuracy: 0.8625 - val_loss: 0.3091\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 875ms/step - accuracy: 0.9547 - loss: 0.1498 - val_accuracy: 0.8600 - val_loss: 0.3205\n",
      "Validation Accuracy for Fold 2: 86.00%\n",
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 878ms/step - accuracy: 0.8961 - loss: 0.2611 - val_accuracy: 0.9600 - val_loss: 0.1371\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 877ms/step - accuracy: 0.9166 - loss: 0.2419 - val_accuracy: 0.9000 - val_loss: 0.2010\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 877ms/step - accuracy: 0.9424 - loss: 0.1934 - val_accuracy: 0.9850 - val_loss: 0.1170\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 883ms/step - accuracy: 0.9526 - loss: 0.1635 - val_accuracy: 0.8750 - val_loss: 0.2890\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 875ms/step - accuracy: 0.9451 - loss: 0.1737 - val_accuracy: 0.8375 - val_loss: 0.3013\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 877ms/step - accuracy: 0.9425 - loss: 0.1778 - val_accuracy: 0.9575 - val_loss: 0.1532\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 895ms/step - accuracy: 0.9468 - loss: 0.1578 - val_accuracy: 0.9200 - val_loss: 0.1900\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 882ms/step - accuracy: 0.9797 - loss: 0.1036 - val_accuracy: 0.9550 - val_loss: 0.1622\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 897ms/step - accuracy: 0.9721 - loss: 0.1095 - val_accuracy: 0.9475 - val_loss: 0.1627\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 894ms/step - accuracy: 0.9577 - loss: 0.1355 - val_accuracy: 0.9375 - val_loss: 0.1838\n",
      "Validation Accuracy for Fold 3: 93.75%\n",
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 888ms/step - accuracy: 0.9230 - loss: 0.2069 - val_accuracy: 0.9575 - val_loss: 0.1176\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 893ms/step - accuracy: 0.9568 - loss: 0.1541 - val_accuracy: 0.9850 - val_loss: 0.0883\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 889ms/step - accuracy: 0.9513 - loss: 0.1597 - val_accuracy: 0.9875 - val_loss: 0.0995\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 881ms/step - accuracy: 0.9647 - loss: 0.1338 - val_accuracy: 0.9450 - val_loss: 0.1494\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 880ms/step - accuracy: 0.9302 - loss: 0.1892 - val_accuracy: 0.9775 - val_loss: 0.1220\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 876ms/step - accuracy: 0.9576 - loss: 0.1507 - val_accuracy: 0.9400 - val_loss: 0.1707\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 877ms/step - accuracy: 0.9618 - loss: 0.1461 - val_accuracy: 0.9275 - val_loss: 0.1921\n",
      "Epoch 8/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 873ms/step - accuracy: 0.9535 - loss: 0.1290 - val_accuracy: 0.9650 - val_loss: 0.1432\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 876ms/step - accuracy: 0.9714 - loss: 0.1150 - val_accuracy: 0.9600 - val_loss: 0.1390\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 880ms/step - accuracy: 0.9588 - loss: 0.1399 - val_accuracy: 0.9575 - val_loss: 0.1508\n",
      "Validation Accuracy for Fold 4: 95.75%\n",
      "Epoch 1/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 876ms/step - accuracy: 0.9284 - loss: 0.1745 - val_accuracy: 0.9900 - val_loss: 0.0764\n",
      "Epoch 2/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 875ms/step - accuracy: 0.9423 - loss: 0.1740 - val_accuracy: 0.9975 - val_loss: 0.0663\n",
      "Epoch 3/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 875ms/step - accuracy: 0.9495 - loss: 0.1519 - val_accuracy: 0.9850 - val_loss: 0.0946\n",
      "Epoch 4/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 876ms/step - accuracy: 0.9758 - loss: 0.1077 - val_accuracy: 0.9875 - val_loss: 0.0785\n",
      "Epoch 5/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 876ms/step - accuracy: 0.9472 - loss: 0.1701 - val_accuracy: 0.9775 - val_loss: 0.1306\n",
      "Epoch 6/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 883ms/step - accuracy: 0.9690 - loss: 0.1219 - val_accuracy: 0.9875 - val_loss: 0.1069\n",
      "Epoch 7/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 876ms/step - accuracy: 0.9702 - loss: 0.1188 - val_accuracy: 0.9875 - val_loss: 0.0924\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 874ms/step - accuracy: 0.9725 - loss: 0.0934 - val_accuracy: 0.9575 - val_loss: 0.1214\n",
      "Epoch 9/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 880ms/step - accuracy: 0.9825 - loss: 0.0762 - val_accuracy: 0.9500 - val_loss: 0.1601\n",
      "Epoch 10/10\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 872ms/step - accuracy: 0.9637 - loss: 0.1043 - val_accuracy: 0.9225 - val_loss: 0.1689\n",
      "Validation Accuracy for Fold 5: 92.25%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_model_multi_class' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 124\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, confusion_matrix, classification_report\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Assuming you have trained the model 'resnet_model' and loaded the test set 'X_val_fold', 'y_val_fold_one_hot'\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m evaluate_model_multi_class(resnet_model, X_val_fold, y_val_fold_one_hot)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate_model_multi_class' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Image dimensions and batch size\n",
    "img_width, img_height = 224, 224\n",
    "batch_size = 8\n",
    "\n",
    "# ... (GPU availability and memory growth code from previous response)\n",
    "def label_images(directory, target_size=(img_width, img_height), max_images_per_class=1000):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_labels = {'Malignant': 0, 'Benign': 1}\n",
    "\n",
    "    for class_label, class_index in class_labels.items():\n",
    "        class_path = os.path.join(directory, class_label)\n",
    "        image_count = 0\n",
    "        for root, _, files in os.walk(class_path):\n",
    "            for filename in files:\n",
    "                if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                    if image_count < max_images_per_class:\n",
    "                        file_path = os.path.join(root, filename)\n",
    "                        image = load_img(file_path, target_size=target_size)  # Load with GPU\n",
    "                        image = img_to_array(image)\n",
    "                        image = preprocess_input(image)  # Process the image after loading\n",
    "                        images.append(image)\n",
    "                        labels.append(class_index)\n",
    "                        image_count += 1\n",
    "                    else:\n",
    "                        break  # Stop processing this class if max images reached\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Example usage:\n",
    "directory_path = 'G:/ChineseCheck/'\n",
    "X, y = label_images(directory_path, target_size=(img_width, img_height))\n",
    "\n",
    "# X contains the resized images, and y contains the corresponding labels\n",
    "print(f\"Total Images: {len(X)}\")\n",
    "print(f\"Shape of an Image: {X[0].shape}\")\n",
    "print(f\"Labels: {y}\")\n",
    "\n",
    "# Split the data into 90% training and 10% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Print the sizes of the training and testing sets\n",
    "print(f\"Training Set: {len(X_train)} samples\")\n",
    "print(f\"Testing Set: {len(X_test)} samples\")\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming you have three classes (malignant, benign, normal)\n",
    "num_classes = 2\n",
    "img_width, img_height = 224, 224  # Adjust these dimensions based on your data\n",
    "\n",
    "def build_resnet_model(learn_rate=0.0001, momentum=0.9):\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "\n",
    "    # Freeze all layers except the last three\n",
    "    for layer in base_model.layers[:-3]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    optimizer = SGD(learning_rate=learn_rate, momentum=momentum)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build a new ResNet50 model\n",
    "resnet_model = build_resnet_model()\n",
    "\n",
    "# Number of folds for k-fold cross-validation\n",
    "num_folds = 5\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=110)\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(X, y), 1):\n",
    "    X_train_fold, X_val_fold = X[train_index], X[val_index]\n",
    "    y_train_fold, y_val_fold = y[train_index], y[val_index]\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    y_train_fold_one_hot = to_categorical(y_train_fold, num_classes=num_classes)\n",
    "    y_val_fold_one_hot = to_categorical(y_val_fold, num_classes=num_classes)\n",
    "\n",
    "    # Train the ResNet50 model\n",
    "    class_weights = {0: 1.0, 1: 3.0}  # Adjust the weights based on class imbalance\n",
    "\n",
    "    history = resnet_model.fit(\n",
    "        X_train_fold,\n",
    "        y_train_fold_one_hot,\n",
    "        epochs=10,\n",
    "        validation_data=(X_val_fold, y_val_fold_one_hot),\n",
    "        batch_size=8,\n",
    "        verbose=1,\n",
    "#         class_weight=class_weights\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss, val_acc = resnet_model.evaluate(X_val_fold, y_val_fold_one_hot, verbose=0)\n",
    "    print(f\"Validation Accuracy for Fold {fold}: {val_acc * 100:.2f}%\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Assuming you have trained the model 'resnet_model' and loaded the test set 'X_val_fold', 'y_val_fold_one_hot'\n",
    "evaluate_model_multi_class(resnet_model, X_val_fold, y_val_fold_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d70c84b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step\n",
      "Accuracy: 92.25%\n",
      "Confusion Matrix:\n",
      "[[179  21]\n",
      " [ 10 190]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92       200\n",
      "           1       0.90      0.95      0.92       200\n",
      "\n",
      "    accuracy                           0.92       400\n",
      "   macro avg       0.92      0.92      0.92       400\n",
      "weighted avg       0.92      0.92      0.92       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Function to calculate and print evaluation metrics for multi-class classification\n",
    "def evaluate_model_multi_class(model, X, y_true):\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Convert one-hot encoding to class labels\n",
    "    y_true_labels = np.argmax(y_true, axis=1)\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true_labels, y_pred_labels)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true_labels, y_pred_labels))\n",
    "\n",
    "# Assuming you have trained the model 'vgg16_model' and loaded the test set 'X_val_fold', 'y_val_fold_one_hot'\n",
    "evaluate_model_multi_class(resnet_model, X_val_fold, y_val_fold_one_hot)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
